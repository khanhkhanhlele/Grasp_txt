{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive5/huypn16/anaconda3/envs/uasd/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-29 04:26:33,641] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "llava-lora-1.5\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ba429de4714f9d86e26b517cb2c914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a3605a1db645678fb2bf2e98ed80c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadrive5/huypn16/anaconda3/envs/uasd/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Loading LoRA weights...\n",
      "Merging LoRA weights...\n",
      "Model is loaded...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from LLaVA.llava.model.builder import load_pretrained_model\n",
    "from LLaVA.llava.mm_utils import get_model_name_from_path\n",
    "\n",
    "from LLaVA.llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from LLaVA.llava.conversation import conv_templates, SeparatorStyle\n",
    "from LLaVA.llava.model.builder import load_pretrained_model\n",
    "from LLaVA.llava.utils import disable_torch_init\n",
    "from LLaVA.llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    "    KeywordsStoppingCriteria,\n",
    ")\n",
    "\n",
    "model_path = \"checkpoints/llava-lora-1.5\"\n",
    "print(get_model_name_from_path(model_path))\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=\"liuhaotian/llava-v1.5-7b\",\n",
    "    model_name=get_model_name_from_path(model_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "query = \"Give me the sunglasses\"\n",
    "image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "if IMAGE_PLACEHOLDER in query:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, query)\n",
    "    else:\n",
    "        qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, query)\n",
    "else:\n",
    "    if model.config.mm_use_im_start_end:\n",
    "        qs = image_token_se + \"\\n\" + query\n",
    "    else:\n",
    "        qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + query\n",
    "conv_mode = \"llava_llama_2\"\n",
    "\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .cuda()\n",
    ")\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n",
    "\n",
    "images = load_images([\"grasp-anything++/seen/image/0e73f01e5d4b1fc064f6ab381209891c376fa26bdc2dbf7578db9b611e6c6337.jpg\"])\n",
    "images_tensor = process_images(\n",
    "    images,\n",
    "    image_processor,\n",
    "    model.config\n",
    ").to(model.device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=images_tensor,\n",
    "        do_sample=True ,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        num_beams=1,\n",
    "        max_new_tokens=40,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_len = input_ids.shape[1]\n",
    "outputs = tokenizer.batch_decode(\n",
    "        output_ids, skip_special_tokens=True\n",
    "    )[0]\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def response_to_batch(inputs, model, eos_token_id, max_length=100):\n",
    "    stop_counter = torch.zeros(len(inputs[\"input_ids\"])).to(inputs[\"input_ids\"].device).long()\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        next_token_logits = []\n",
    "        outputs = model(**inputs)\n",
    "        token_logits = outputs.logits[:, -1, :]\n",
    "        last_hidden_states = outputs.hidden_states[-1]\n",
    "        next_token_logits.append(token_logits)\n",
    "        next_token_logits = torch.cat(next_token_logits, dim=-1)\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "        input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "        \n",
    "        inputs[\"input_ids\"] = input_ids\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1)).to(attention_mask.device)], dim=-1).long()\n",
    "        inputs[\"attention_mask\"] = attention_mask\n",
    "        \n",
    "        stop_counter += (next_token == eos_token_id).detach().long()\n",
    "        \n",
    "        if (stop_counter > 0).all():\n",
    "            break\n",
    "        \n",
    "        print(last_hidden_states)\n",
    "        \n",
    "    return input_ids[-max_length:]\n",
    "\n",
    "output = response_to_batch(inputs, model, 32001)\n",
    "\n",
    "generated_text = processor.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor([\"Here is [SPT] handle of the mug [SPT]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"grasp-anything++/seen/grasp_instructions/1cdd8f145672b4a7959503e815d3bb67ede9f7e9fb7c7be8d9f743f04bbf5bc7_0_0.pkl\", \"rb\") as f:\n",
    "    instruction = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uasd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
