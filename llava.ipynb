{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'LLaVA.llava'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLLaVA\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pretrained_model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLLaVA\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmm_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model_name_from_path\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mLLaVA\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     IMAGE_TOKEN_INDEX,\n\u001b[1;32m      7\u001b[0m     DEFAULT_IMAGE_TOKEN,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     IMAGE_PLACEHOLDER,\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'LLaVA.llava'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from LLaVA.llava.model.builder import load_pretrained_model\n",
    "from LLaVA.llava.mm_utils import get_model_name_from_path\n",
    "\n",
    "from LLaVA.llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from LLaVA.llava.conversation import SeparatorStyle\n",
    "from LLaVA.llava.model.builder import load_pretrained_model\n",
    "from LLaVA.llava.utils import disable_torch_init\n",
    "from LLaVA.llava.mm_utils import (\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    get_model_name_from_path,\n",
    "    KeywordsStoppingCriteria,\n",
    ")\n",
    "\n",
    "model_path = \"checkpoints/llava-lora-1.10\"\n",
    "print(get_model_name_from_path(model_path))\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=\"liuhaotian/llava-v1.5-7b\",\n",
    "    model_name=get_model_name_from_path(model_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Descibe the image\"\n",
    "prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_ids = (\n",
    "    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "    .unsqueeze(0)\n",
    "    .cuda()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def load_image(image_file):\n",
    "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
    "        response = requests.get(image_file)\n",
    "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    else:\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def load_images(image_files):\n",
    "    out = []\n",
    "    for image_file in image_files:\n",
    "        image = load_image(image_file)\n",
    "        out.append(image)\n",
    "    return out\n",
    "\n",
    "images = load_images([\"grasp-anything++/seen/image/0b40406618b99611efd8ca8798e8ce46f4951dc733c1fc17adf94f8b6afec999.jpg\"])\n",
    "images_tensor = process_images(\n",
    "    images,\n",
    "    image_processor,\n",
    "    model.config\n",
    ").to(model.device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=images_tensor,\n",
    "        do_sample=False,\n",
    "        temperature=0.8,\n",
    "        max_new_tokens=100,\n",
    "        use_cache=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 29889,    13,    13, 29902,   871,  3867,   278,  3646, 25274,\n",
       "          1203,  1024,  2729,   373,   278,  1881,  1347, 29889,   512,   445,\n",
       "          1206, 29892,   278,  3646, 25274,  1203,  1024,   338,   376,  1731,\n",
       "           261,  1642,   518,  5550, 29911, 29962, 28149,   518,  5550, 29911,\n",
       "         29962, 29871,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\n\\nI only provide the target grasp object name based on the input string. In this case, the target grasp object name is \"flower\". [SPT] flower [SPT] '"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_len = input_ids.shape[1]\n",
    "outputs = tokenizer.batch_decode(\n",
    "        output_ids, skip_special_tokens=True\n",
    "    )[0]\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,    13,    13, 10605,   338,   518,  5550, 29911, 29962, 26163,\n",
       "           518,  5550, 29911, 29962, 29871,     2]], device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vision_tower_name = \"openai/clip-vit-large-patch14-336\"\n",
    "\n",
    "processor = CLIPImageProcessor.from_pretrained(vision_tower_name)\n",
    "vision_tower = CLIPVisionModel.from_pretrained(vision_tower_name, device_map=\"auto\")\n",
    "image_forward_out = vision_tower(images_tensor[0].to(device=device).unsqueeze(0), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_forward_out.last_hidden_state.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uasd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
